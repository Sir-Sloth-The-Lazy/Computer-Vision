# -*- coding: utf-8 -*-
"""Weights&Biases.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aNKKnzlE48ofb5ieojrgPJ1ce6VV_hUY
"""

import wandb
from wandb.integration.keras import WandbMetricsLogger , WandbModelCheckpoint

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras

sweep_config = {
    'method' : 'grid',
    'metric' : {
        'name' : 'val_accuracy',
        'goal' : 'maximize'
    },
    'parameters' : {
        'batch_size' : {'values' : [8,16]},
        'learning_rate' : {'values' : [0.001, 0.0001]},
        'hidden_nodes' :{'values' : [128 , 64]},
        'img_size': {'values' : [16, 224]},
        'epochs' : {'values' : [5,10]}
    }
}

sweep_id = wandb.sweep(sweep_config , project= "5-Flowers-Dataset")

def train():
  with wandb.init() as run:
    config = wandb.config

    IMG_HEIGHT = config.img_size
    IMG_WIDTH = config.img_size
    IMG_CHANNELS = 3

    CLASS_NAMES = ["daisy" , "dandelion" , "roses" , "sunflowers" , "tulips"]

    def read_and_decode(filename, resize_dims):
      img_bytes = tf.io.read_file(filename)
      img = tf.image.decode_jpeg(img_bytes , channels = IMG_CHANNELS)
      img = tf.image.convert_image_dtype(img , tf.float32)
      img = tf.image.resize(img , resize_dims)
      return img

    def parse_csvline(csv_line):
      record_default = ["" , "" ]
      filename , label_string = tf.io.decode_csv(csv_line , record_default)

      img = read_and_decode(filename , [IMG_HEIGHT , IMG_WIDTH])

      #convert the label_string to integer
      label = tf.where(tf.equal(CLASS_NAMES , label_string))[0, 0]

      return img, label

    train_dataset = (
        tf.data.TextLineDataset("gs://cloud-ml-data/img/flower_photos/train_set.csv")
        .map(parse_csvline , num_parallel_calls= tf.data.AUTOTUNE)
        .batch(config.batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )

    eval_dataset = (
        tf.data.TextLineDataset("gs://cloud-ml-data/img/flower_photos/train_set.csv")
        .map(parse_csvline , num_parallel_calls= tf.data.AUTOTUNE)
        .batch(config.batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )

    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),
        keras.layers.Dense(config.hidden_nodes, activation="relu"),
        keras.layers.Dense(len(CLASS_NAMES) , activation="softmax")
    ])

    model.compile(
        optimizer = "adam",
        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
        metrics=["accuracy"]
    )

    model.fit(
        train_dataset,
        validation_data=eval_dataset,
        epochs=config.epochs,
        callbacks=[WandbMetricsLogger(log_freq=5)]
    )

wandb.agent(sweep_id, function=train)